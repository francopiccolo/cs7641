{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from itertools import product\n",
    "from time import clock\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import scipy.sparse as sps\n",
    "from scipy.linalg import pinv\n",
    "from scipy.spatial import distance \n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.random_projection import SparseRandomProjection, GaussianRandomProjection\n",
    "from sklearn.cluster import KMeans as kmeans\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.feature_selection import mutual_info_classif as MIC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import adjusted_mutual_info_score as ami, accuracy_score as accuracy, pairwise_distances\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def cluster_acc(Y, clusterLabels):\n",
    "    assert (Y.shape == clusterLabels.shape)\n",
    "    pred = np.empty_like(Y)\n",
    "    for label in set(clusterLabels):\n",
    "        mask = clusterLabels == label\n",
    "        sub = Y[mask]\n",
    "        target = Counter(sub).most_common(1)[0][0]\n",
    "        pred[mask] = target\n",
    "#    assert max(pred) == max(Y)\n",
    "#    assert min(pred) == min(Y)    \n",
    "    return accuracy(Y, pred)\n",
    "\n",
    "\n",
    "class myGMM(GMM):\n",
    "    def transform(self, X):\n",
    "        return self.predict_proba(X)\n",
    "        \n",
    "        \n",
    "def pairwiseDistCorr(X1, X2):\n",
    "    assert X1.shape[0] == X2.shape[0]\n",
    "    \n",
    "    d1 = pairwise_distances(X1)\n",
    "    d2 = pairwise_distances(X2)\n",
    "    return np.corrcoef(d1.ravel(), d2.ravel())[0,1]\n",
    "\n",
    "    \n",
    "def aveMI(X, Y):    \n",
    "    MI = MIC(X, Y) \n",
    "    return np.nanmean(MI)\n",
    "    \n",
    "  \n",
    "def reconstructionError(projections, X):\n",
    "    W = projections.components_\n",
    "    if sps.issparse(W):\n",
    "        W = W.todense()\n",
    "    p = pinv(W)\n",
    "    reconstructed = ((p@W)@(X.T)).T # Unproject projected data\n",
    "    errors = np.square(X-reconstructed)\n",
    "    return np.nanmean(errors)\n",
    "    \n",
    "# http://datascience.stackexchange.com/questions/6683/feature-selection-using-feature-importances-in-random-forests-with-scikit-learn          \n",
    "class ImportanceSelect(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model, n=1):\n",
    "         self.model = model\n",
    "         self.n = n\n",
    "    def fit(self, *args, **kwargs):\n",
    "         self.model.fit(*args, **kwargs)\n",
    "         return self\n",
    "    def transform(self, X):\n",
    "         return X[:,self.model.feature_importances_.argsort()[::-1][:self.n]]\n",
    "                  \n",
    "#http://stats.stackexchange.com/questions/90769/using-bic-to-estimate-the-number-of-k-in-kmeans    \n",
    "def compute_bic(kmeans, X):\n",
    "    \"\"\"\n",
    "    Computes the BIC metric for a given clusters\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "    kmeans:  List of clustering object from scikit learn\n",
    "    X     :  multidimension np array of data points\n",
    "    Returns:\n",
    "    -----------------------------------------\n",
    "    BIC value\n",
    "    \"\"\"\n",
    "    # assign centers and labels\n",
    "    centers = [kmeans.cluster_centers_]\n",
    "    labels  = kmeans.labels_\n",
    "    #number of clusters\n",
    "    m = kmeans.n_clusters\n",
    "    # size of the clusters\n",
    "    n = np.bincount(labels)\n",
    "    #size of data set\n",
    "    N, d = X.shape\n",
    "\n",
    "    #compute variance for all clusters beforehand\n",
    "    cl_var = (1.0 / (N - m) / d) * sum([sum(distance.cdist(X[np.where(labels == i)], [centers[0][i]], 'euclidean')**2) for i in range(m)])\n",
    "\n",
    "    const_term = 0.5 * m * np.log(N) * (d+1)\n",
    "\n",
    "    BIC = np.sum([n[i] * np.log(n[i]) -\n",
    "               n[i] * np.log(N) -\n",
    "             ((n[i] * d) / 2) * np.log(2*np.pi*cl_var) -\n",
    "             ((n[i] - 1) * d/ 2) for i in range(m)]) - const_term\n",
    "\n",
    "    return(BIC)\n",
    "\n",
    "def plot_metric_vs_comp(df, problem_name, multiple_runs,\n",
    "                        title, ylabel):\n",
    "\n",
    "    plt.close()\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"N Components\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    x_points = df.index.values\n",
    "    y_points = df[1]\n",
    "    if multiple_runs:\n",
    "        y_points = np.mean(df.iloc[:, 1: -1], axis=1)\n",
    "        y_std = np.std(df.iloc[:, 1: -1], axis=1)\n",
    "        plt.plot(x_points, y_points, 'o-', linewidth=1, markersize=2,\n",
    "                 label=ylabel)\n",
    "        plt.fill_between(x_points, y_points - y_std,\n",
    "                         y_points + y_std, alpha=0.2)\n",
    "    else:\n",
    "        plt.plot(x_points, y_points, 'o-', linewidth=1, markersize=2,\n",
    "                 label=ylabel)\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "def run_plot_metric_vs_comp(out, dataset, metric, problem, title):\n",
    "    file = out + '{}_{}.csv'.format(dataset, metric)\n",
    "    df = pd.read_csv(file, header=None).dropna().set_index(0)\n",
    "    multiple_runs = True if problem == 'RP' else False\n",
    "    p = plot_metric_vs_comp(df=df, problem_name=problem, multiple_runs=multiple_runs,\n",
    "                            title=title, ylabel=metric)\n",
    "\n",
    "    p.savefig('{}/{}_{}.png'.format(out, dataset, metric),\n",
    "                                    format='png', bbox_inches='tight', dpi=150)\n",
    "\n",
    "def plot_complexity_curve(title, param, problem, classifier, dataset, ylim=None):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    \n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    grid_search = pd.read_csv('./{}/{}_cv_results.csv'.format(problem, dataset))\n",
    "    best_columns = ['mean_train_score', 'mean_test_score', 'mean_fit_time', 'mean_score_time', 'params']\n",
    "    best = pd.DataFrame(grid_search[best_columns].loc[grid_search.mean_test_score.idxmax()]).T.reset_index()\n",
    "    best.to_csv('./{}/{}_best.csv'.format(problem, dataset))\n",
    "    best_params = ast.literal_eval(best.loc[0, 'params'])\n",
    "\n",
    "    best_params.pop('{}__{}'.format(classifier, param))\n",
    "    for _param, value in best_params.items():\n",
    "        if isinstance(value, tuple):\n",
    "            grid_search['param_'+_param] = grid_search['param_'+_param].apply(ast.literal_eval)\n",
    "        grid_search = grid_search.loc[grid_search['param_'+_param] == value]\n",
    "\n",
    "    df = grid_search[['param_{}__{}'.format(classifier, param), \n",
    "                      'mean_test_score', \n",
    "                      'std_test_score', \n",
    "                      'mean_train_score', \n",
    "                      'std_train_score']].sort_values(by='param_{}__{}'.format(classifier, param))\n",
    "    param_values = df['param_{}__{}'.format(classifier, param)]\n",
    "    train_scores_mean = df['mean_train_score']\n",
    "    train_scores_std = df['std_train_score']\n",
    "    test_scores_mean = df['mean_test_score']\n",
    "    test_scores_std = df['std_test_score']\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(param_values, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.fill_between(param_values, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2)\n",
    "    plt.plot(param_values, train_scores_mean, 'o-', linewidth=1, markersize=4,\n",
    "             label=\"Train\")\n",
    "    plt.plot(param_values, test_scores_mean, 'o-', linewidth=1, markersize=4,\n",
    "             label=\"Test\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "def make_complexity_curve(clf_name, dataset, param, problem):    \n",
    "    print('Making complexity curve for', dataset)\n",
    "    plt = plot_complexity_curve('Complexity Curve: {} - {} - {}'.format(clf_name, dataset, param),\n",
    "                                param,\n",
    "                                problem,\n",
    "                                clf_name,\n",
    "                                dataset,\n",
    "                                ylim=None)\n",
    "\n",
    "    plt.savefig('./{}/{}_CC_{}.png'.format(problem, dataset, param), format='png', dpi=150)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making complexity curve for wine\n"
     ]
    }
   ],
   "source": [
    "make_complexity_curve('ica', 'wine', 'n_components', 'ICA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
