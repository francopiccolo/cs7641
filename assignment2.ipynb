{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import mlrose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def balanced_accuracy(truth, pred):\n",
    "    wts = compute_sample_weight('balanced', truth)\n",
    "    return accuracy_score(truth, pred, sample_weight=wts)\n",
    "\n",
    "scorer = make_scorer(balanced_accuracy)    \n",
    "\n",
    "# Plots learning curve\n",
    "def plot_learning_curve(title,\n",
    "                        train_sizes,\n",
    "                        train_scores, test_scores, ylim=None, \n",
    "                        cv=None,\n",
    "                        n_jobs=None):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Test score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Grid search + learning curve\n",
    "def basicResults(clfObj, trgX, trgY, tstX, tstY, params, clf_type=None, dataset=None):\n",
    "    np.random.seed(55)\n",
    "    if clf_type is None or dataset is None:\n",
    "        raise\n",
    "    print('Calculating basic results for', dataset)\n",
    "    \n",
    "    # Saves results of hyperparameters grid search\n",
    "    cv = GridSearchCV(clfObj, \n",
    "                     n_jobs=-1, \n",
    "                     param_grid=params,\n",
    "                     verbose=False,\n",
    "                     cv=5, \n",
    "                     scoring=scorer, \n",
    "                     return_train_score=True)\n",
    "    cv.fit(trgX, trgY)\n",
    "    st = time()\n",
    "    cv.best_estimator_.fit(trgX, trgY)\n",
    "    train_time = time() - st\n",
    "    st = time()\n",
    "    cv.predict(trgX[0].reshape(1, -1))\n",
    "    predict_time = time() - st\n",
    "    \n",
    "    def schedule_to_str(schedule):\n",
    "        schedule = str(schedule)\n",
    "        if 'Geom' in schedule:\n",
    "            return 'geom_decay'\n",
    "        elif 'Exp' in schedule:\n",
    "            return 'exp_decay'\n",
    "        elif 'Arith' in schedule:\n",
    "            return 'arith_decay'\n",
    "    \n",
    "    best_params = cv.best_params_.copy()\n",
    "    regTable = pd.DataFrame(cv.cv_results_)\n",
    "    \n",
    "    if 'NN_sa__schedule' in params:\n",
    "        regTable['param_NN_sa__schedule'] = regTable['param_NN_sa__schedule'].apply(lambda x: schedule_to_str(x))\n",
    "        best_params['NN_sa__schedule'] = schedule_to_str(best_params['NN_sa__schedule'])\n",
    "    \n",
    "    regTable.to_csv('./output/{}_{}_reg.csv'.format(clf_type, dataset), index=False)\n",
    "    train_score = cv.score(trgX, trgY)\n",
    "    test_score = cv.score(tstX, tstY)\n",
    "\n",
    "    # Saves best iteration\n",
    "    results = pd.DataFrame({'classifier': [clf_type],\n",
    "                 'dataset': [dataset],\n",
    "                 'train_score': [train_score],\n",
    "                 'test_score': [test_score],\n",
    "                 'train_time': [train_time],\n",
    "                 'predict_time': [predict_time],\n",
    "                 'params': [best_params]})\n",
    "    \n",
    "    results.to_csv('./output/{}_{}_best.csv'.format(clf_type, dataset))\n",
    "    \n",
    "    # Saves learning curve\n",
    "    N = trgY.shape[0]\n",
    "    train_sizes = [50,100]+[int(N*x/10) for x in range(1,8)]\n",
    "    np.random.seed(55)\n",
    "    curve = learning_curve(cv.best_estimator_,\n",
    "                           trgX, trgY,\n",
    "                           cv=5,\n",
    "                           train_sizes=train_sizes,\n",
    "                           verbose=False,\n",
    "                           scoring=scorer)\n",
    "\n",
    "    curve_train_scores = pd.DataFrame(index=curve[0], data=curve[1])\n",
    "    curve_test_scores  = pd.DataFrame(index=curve[0], data=curve[2])\n",
    "    \n",
    "    curve_train_scores.to_csv('./output/{}_{}_LC_train.csv'.format(clf_type, dataset))\n",
    "    curve_test_scores.to_csv('./output/{}_{}_LC_test.csv'.format(clf_type, dataset))\n",
    "    \n",
    "    plt = plot_learning_curve('Learning Curve: {} - {}'.format(clf_type, dataset),\n",
    "                        train_sizes,\n",
    "                        curve_train_scores, curve_test_scores, ylim=None, \n",
    "                        cv=None,\n",
    "                        n_jobs=None)\n",
    "    \n",
    "    plt.savefig('./output/images/{}_{}_LC.png'.format(clf_type, dataset), format='png', dpi=150)\n",
    "    plt.close()\n",
    "    return cv\n",
    "\n",
    "def iterationLC(clfObj, trgX, trgY, tstX, tstY, params, clf_type=None, dataset=None):\n",
    "    print('Calculating iteration learning curve for ',dataset)\n",
    "    np.random.seed(55)\n",
    "    if clf_type is None or dataset is None:\n",
    "        raise\n",
    "    \n",
    "    d = defaultdict(list)\n",
    "    name = list(params.keys())[0]\n",
    "    for value in list(params.values())[0]:        \n",
    "        d['param_{}'.format(name)].append(value)\n",
    "        clfObj.set_params(**{name: value})\n",
    "        clfObj.fit(trgX, trgY)\n",
    "        pred = clfObj.predict(trgX)\n",
    "        d['train acc'].append(balanced_accuracy(trgY, pred))\n",
    "        clfObj.fit(trgX, trgY)\n",
    "        pred = clfObj.predict(tstX)\n",
    "        d['test acc'].append(balanced_accuracy(tstY, pred))\n",
    "        \n",
    "    d = pd.DataFrame(d)\n",
    "    d.to_csv('./output/{}_{}_ILC.csv'.format(clf_type, dataset), index=False)\n",
    "    return\n",
    "    \n",
    "# Timing curve\n",
    "def plot_timing_curve(title, data_sizes, fit_times, predict_times, ylim=None):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Testing Size (% of total)\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "    predict_times_mean = np.mean(predict_times, axis=1)\n",
    "    predict_times_std = np.std(predict_times, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(data_sizes, fit_times_mean - fit_times_std,\n",
    "                     fit_times_mean + fit_times_std, alpha=0.2)\n",
    "    plt.fill_between(data_sizes, predict_times_mean - predict_times_std,\n",
    "                     predict_times_mean + predict_times_std, alpha=0.2)\n",
    "    plt.plot(data_sizes, predict_times_mean, 'o-', linewidth=1, markersize=4,\n",
    "             label=\"Predict time\")\n",
    "    plt.plot(data_sizes, fit_times_mean, 'o-', linewidth=1, markersize=4,\n",
    "             label=\"Fit time\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "def make_timing_curve(X, Y, clf, clf_name, dataset):\n",
    "    out = defaultdict(dict)\n",
    "    print('Making timing curve for', dataset)\n",
    "    fracs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    for frac in fracs:  \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=frac, random_state=42)\n",
    "        st = time()\n",
    "        np.random.seed(55)\n",
    "        clf.fit(X_train, y_train)\n",
    "        out['train'][frac]= time() - st\n",
    "        st = time()\n",
    "        clf.predict(X_test)\n",
    "        out['test'][frac]= time() - st\n",
    "    out = pd.DataFrame(out)\n",
    "    out.to_csv('./output/{}_{}_timing.csv'.format(clf_name, dataset))\n",
    "    train_df = pd.DataFrame(out['train'], index=fracs)\n",
    "    test_df = pd.DataFrame(out['test'], fracs)\n",
    "    plt = plot_timing_curve('Timing Curve: {} - {}'.format(clf_name, dataset),\n",
    "                            np.array(fracs) * 100, \n",
    "                            train_df, test_df)\n",
    "    \n",
    "    plt.savefig('./output/images/{}_{}_TC.png'.format(clf_name, dataset), format='png', dpi=150)\n",
    "    plt.close()\n",
    "    return\n",
    "\n",
    "def plot_complexity_curve(title, param, classifier, dataset, ylim=None):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    \n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    best = pd.read_csv('./output/{}_{}_best.csv'.format(classifier, dataset))\n",
    "    best_params = ast.literal_eval(best.loc[0, 'params'])\n",
    "#     print(type(best_params['NN_ga__learning_rate']))\n",
    "    grid_search = pd.read_csv('./output/{}_{}_reg.csv'.format(classifier, dataset))\n",
    "#     print(grid_search['param_NN_ga__learning_rate'].dtype)\n",
    "#     print(grid_search['param_NN_ga__learning_rate'])\n",
    "    best_params.pop('{}__{}'.format(classifier, param))\n",
    "    for _param, value in best_params.items():\n",
    "        if isinstance(value, tuple):\n",
    "            grid_search['param_'+_param] = grid_search['param_'+_param].apply(ast.literal_eval)\n",
    "        grid_search = grid_search.loc[grid_search['param_'+_param] == value]\n",
    "#         print(_param, value, grid_search.iloc[0:2])\n",
    "\n",
    "    df = grid_search[['param_{}__{}'.format(classifier, param), \n",
    "                      'mean_test_score', \n",
    "                      'std_test_score', \n",
    "                      'mean_train_score', \n",
    "                      'std_train_score']].sort_values(by='param_{}__{}'.format(classifier, param))\n",
    "    param_values = df['param_{}__{}'.format(classifier, param)]\n",
    "    train_scores_mean = df['mean_train_score']\n",
    "    train_scores_std = df['std_train_score']\n",
    "    test_scores_mean = df['mean_test_score']\n",
    "    test_scores_std = df['std_test_score']\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(param_values, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2)\n",
    "    plt.fill_between(param_values, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2)\n",
    "    plt.plot(param_values, train_scores_mean, 'o-', linewidth=1, markersize=4,\n",
    "             label=\"Train\")\n",
    "    plt.plot(param_values, test_scores_mean, 'o-', linewidth=1, markersize=4,\n",
    "             label=\"Test\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "def make_complexity_curve(clf_name, dataset, param):    \n",
    "    print('Making complexity curve for', dataset)\n",
    "    plt = plot_complexity_curve('Complexity Curve: {} - {} - {}'.format(clf_name, dataset, param),\n",
    "                                param,\n",
    "                                clf_name,\n",
    "                                dataset,\n",
    "                                ylim=None)\n",
    "\n",
    "    plt.savefig('./output/images/{}_{}_CC_{}.png'.format(clf_name, dataset, param), format='png', dpi=150)\n",
    "    plt.close()\n",
    "    return\n",
    "\n",
    "def plot_iteration_learning_curve(title, param, classifier, dataset, ylim=None):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    \n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "        \n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    df = pd.read_csv('./output/{}_{}_ILC.csv'.format(classifier, dataset))\n",
    "    param_values = df['param_{}__{}'.format(classifier, param)]\n",
    "    train_scores = df['train acc']\n",
    "    test_scores = df['test acc']\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.plot(param_values, train_scores, \n",
    "             'o-', linewidth=1, markersize=4,\n",
    "             label=\"Train\")\n",
    "    plt.plot(param_values, test_scores, \n",
    "             'o-', linewidth=1, markersize=4,\n",
    "             label=\"Test\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "def make_iteration_learning_curve(clf_name, dataset, param):    \n",
    "    print('Making iteration learning curve for', dataset)\n",
    "    plt = plot_iteration_learning_curve('Iteration Learning Curve: {} - {}'.format(clf_name, dataset),\n",
    "                                        param,\n",
    "                                        clf_name,\n",
    "                                        dataset,\n",
    "                                        ylim=None)\n",
    "\n",
    "    plt.savefig('./output/images/{}_{}_ILC.png'.format(clf_name, dataset), format='png', dpi=150)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (4898, 12)\n",
      "Quality values:  Index(['bad', 'good'], dtype='object')\n",
      "Labels balance: \n",
      " bad     0.783585\n",
      "good    0.216415\n",
      "Name: quality_label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Wine dataset\n",
    "\n",
    "wine = pd.read_csv('winequality-white.csv', sep = ';')\n",
    "print('Dataset shape: ', str(wine.shape))\n",
    "\n",
    "# There are no null values in the dataset\n",
    "\n",
    "# Creation of class for good vs bad wines (good when greater or equal than 7)\n",
    "wine['quality_label'] = 'bad'\n",
    "wine.loc[wine['quality']>=7, 'quality_label'] = 'good'\n",
    "wine['quality_label'] = wine['quality_label'].astype('category')\n",
    "\n",
    "print('Quality values: ', str(wine['quality_label'].cat.categories))\n",
    "print('Labels balance: \\n', wine['quality_label'].value_counts()/wine['quality_label'].size)\n",
    "\n",
    "wine['quality_int'] = wine['quality']\n",
    "wine['quality'] = wine['quality_label'].cat.codes\n",
    "\n",
    "wine = wine.drop(['quality_label', 'quality_int'], axis=1)\n",
    "wine = wine.astype(float)\n",
    "wine.quality = wine.quality.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data       \n",
    "wineX = wine.drop('quality', axis=1).values\n",
    "wineY = wine['quality'].values\n",
    "\n",
    "wine_trgX, wine_tstX, wine_trgY, wine_tstY = train_test_split(wineX, wineY, test_size=0.3, random_state=0, stratify=wineY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Networks\n",
    "class NeuralNetworkF(mlrose.NeuralNetwork):\n",
    "    def get_params(self, deep):\n",
    "        return {'hidden_nodes': self.hidden_nodes,\n",
    "                'max_iters': self.max_iters,\n",
    "                'bias': self.bias,\n",
    "                'is_classifier': self.is_classifier,\n",
    "                'learning_rate': self.lr,\n",
    "                'early_stopping': self.early_stopping,\n",
    "                'clip_max': self.clip_max,\n",
    "                'schedule': self.schedule,\n",
    "                'pop_size': self.pop_size,\n",
    "                'mutation_prob': self.mutation_prob}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "nn_params = {'hidden_nodes': [22],\n",
    "             'activation': 'relu',\n",
    "             'bias': True,\n",
    "             'is_classifier': True,\n",
    "             'early_stopping': True,\n",
    "             'learning_rate': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating basic results for wine\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-1aa6e3da5220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m wine_clf = basicResults(pipeW, wine_trgX, wine_trgY, wine_tstX, wine_tstY, \n\u001b[0;32m---> 13\u001b[0;31m                         params_wine, 'NN_rhc', 'wine')        \n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmake_complexity_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NN_rhc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-9150809dc192>\u001b[0m in \u001b[0;36mbasicResults\u001b[0;34m(clfObj, trgX, trgY, tstX, tstY, params, clf_type, dataset)\u001b[0m\n\u001b[1;32m     54\u001b[0m                      \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                      return_train_score=True)\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrgX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrgY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrgX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrgY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/francopiccolo/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    636\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    637\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 638\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/francopiccolo/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/francopiccolo/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/francopiccolo/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/francopiccolo/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/francopiccolo/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/francopiccolo/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Neural Networks random hill climb\n",
    "pipeW = Pipeline([('Scale', StandardScaler()),\n",
    "                  ('NN_rhc', NeuralNetworkF(algorithm='random_hill_climb',\n",
    "                                        **nn_params))])\n",
    "\n",
    "learning_rate = [10**x for x in np.arange(-4, 0, 1/2)]\n",
    "\n",
    "params_wine = {'NN_rhc__max_iters': [5000],\n",
    "               'NN_rhc__max_attempts': [50],\n",
    "               'NN_rhc__learning_rate': learning_rate}\n",
    "\n",
    "wine_clf = basicResults(pipeW, wine_trgX, wine_trgY, wine_tstX, wine_tstY, \n",
    "                        params_wine, 'NN_rhc', 'wine')        \n",
    "\n",
    "make_complexity_curve('NN_rhc', 'wine', 'learning_rate')\n",
    "\n",
    "wine_final_params = wine_clf.best_params_\n",
    "\n",
    "pipeW.set_params(**wine_final_params)\n",
    "make_timing_curve(wineX, wineY, pipeW, 'NN_rhc', 'wine')\n",
    "\n",
    "max_iters = [2**x for x in range(14)]\n",
    "pipeW.set_params(**{'NN_rhc__early_stopping': False})    \n",
    "iterationLC(pipeW, wine_trgX, wine_trgY, wine_tstX, wine_tstY, \n",
    "            {'NN_rhc__max_iters': max_iters}, \n",
    "             'NN_rhc', 'wine')\n",
    "make_iteration_learning_curve('NN_rhc', 'wine', 'max_iters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating basic results for wine\n"
     ]
    }
   ],
   "source": [
    "# Neural Networks simulated annealing\n",
    "pipeW = Pipeline([('Scale', StandardScaler()),\n",
    "                  ('NN_sa', NeuralNetworkF(algorithm='simulated_annealing',\n",
    "                                        **nn_params))])\n",
    "\n",
    "learning_rate = [10**x for x in np.arange(-4, 1/2, 1/2)]\n",
    "schedule = [mlrose.GeomDecay(), mlrose.ArithDecay(), mlrose.ExpDecay()]\n",
    "\n",
    "params_wine = {'NN_sa__max_iters': [5000],\n",
    "               'NN_sa__max_attempts': [50],\n",
    "               'NN_sa__learning_rate': learning_rate,\n",
    "               'NN_sa__schedule': schedule}\n",
    "\n",
    "wine_clf = basicResults(pipeW, wine_trgX, wine_trgY, wine_tstX, wine_tstY, \n",
    "                        params_wine, 'NN_sa', 'wine')        \n",
    "\n",
    "make_complexity_curve('NN_sa', 'wine', 'learning_rate')\n",
    "\n",
    "wine_final_params = wine_clf.best_params_\n",
    "\n",
    "pipeW.set_params(**wine_final_params)\n",
    "make_timing_curve(wineX, wineY, pipeW, 'NN_sa', 'wine')\n",
    "\n",
    "# max_iters = [2**x for x in range(14)]\n",
    "max_iters = [2**x for x in range(12)]\n",
    "pipeW.set_params(**{'NN_sa__early_stopping': False})    \n",
    "iterationLC(pipeW, wine_trgX, wine_trgY, wine_tstX, wine_tstY, \n",
    "            {'NN_sa__max_iters': max_iters}, \n",
    "             'NN_sa', 'wine')\n",
    "make_iteration_learning_curve('NN_sa', 'wine', 'max_iters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating basic results for wine\n"
     ]
    }
   ],
   "source": [
    "# Neural Networks Genetic Algorithm\n",
    "pipeW = Pipeline([('Scale', StandardScaler()),\n",
    "                  ('NN_ga', NeuralNetworkF(algorithm='genetic_alg',\n",
    "                                        **nn_params))])\n",
    "\n",
    "learning_rate = [round(10**x, 8) for x in np.arange(-4, 0, 1/2)]\n",
    "mutation_prob = [round(10**x, 8) for x in np.arange(-4, 0, 1/2)]\n",
    "pop_size = [round(10**x) for x in np.arange(1, 3.5, 1/2)]\n",
    "\n",
    "params_wine = {'NN_ga__max_iters': [2000],\n",
    "               'NN_ga__max_attempts': [20],\n",
    "               'NN_ga__learning_rate': learning_rate,\n",
    "               'NN_ga__pop_size': pop_size,\n",
    "               'NN_ga__mutation_prob': mutation_prob}\n",
    "\n",
    "wine_clf = basicResults(pipeW, wine_trgX, wine_trgY, wine_tstX, wine_tstY, \n",
    "                        params_wine, 'NN_ga', 'wine')        \n",
    "\n",
    "make_complexity_curve('NN_ga', 'wine', 'learning_rate')\n",
    "\n",
    "wine_final_params = wine_clf.best_params_\n",
    "\n",
    "pipeW.set_params(**wine_final_params)\n",
    "make_timing_curve(wineX, wineY, pipeW, 'NN_ga', 'wine')\n",
    "\n",
    "max_iters = [2**x for x in range(12)]\n",
    "pipeW.set_params(**{'NN_ga__early_stopping': False})\n",
    "iterationLC(pipeW, wine_trgX, wine_trgY, wine_tstX, wine_tstY, \n",
    "            {'NN_ga__max_iters': max_iters}, \n",
    "             'NN_ga', 'wine')\n",
    "make_iteration_learning_curve('NN_ga', 'wine', 'max_iters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max 1's\n",
    "# Knapsack Problem\n",
    "# Traveller problem\n",
    "# 4 peaks problem\n",
    "# Max colors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
